{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NtibXctgCmhV"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kan-bayashi/ParallelWaveGAN/blob/master/notebooks/convert_melgan_from_pytorch_to_tensorflow.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPtDpfkQ9R8G"
   },
   "source": [
    "# Convert MelGAN generator from pytorch to tensorflow\n",
    "\n",
    "This notebook proivdies the procedure of conversion of MelGAN generator from pytorch to tensorflow.  \n",
    "Tensorflow version can accelerate the inference speed on both CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 5AFE-AE81\n",
      "\n",
      " Directory of C:\\Users\\xzten\\Desktop\\Traits\\ParallelWaveGAN\\notebooks\n",
      "\n",
      "12/08/2020  01:23 PM    <DIR>          .\n",
      "12/08/2020  01:23 PM    <DIR>          ..\n",
      "12/08/2020  01:16 PM    <DIR>          .ipynb_checkpoints\n",
      "12/08/2020  01:23 PM            20,380 convert_melgan_from_pytorch_to_tensorflow.ipynb\n",
      "12/08/2020  01:00 PM         5,440,009 filter_design_example.ipynb\n",
      "12/08/2020  01:20 PM    <DIR>          ParallelWaveGAN\n",
      "               2 File(s)      5,460,389 bytes\n",
      "               4 Dir(s)  867,599,093,760 bytes free\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "dB-N-PJU9Txg",
    "outputId": "e6721858-9f61-4006-9b2f-40b45f8d9b3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\n"
     ]
    }
   ],
   "source": [
    "# install libraries for google colab\n",
    "# !git clone https://github.com/kan-bayashi/ParallelWaveGAN.git\n",
    "!cd ..\n",
    "!pip install -qq .\n",
    "!pip install -qq tensorflow-gpu==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwYMtOUs9R8I"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-847499fd07a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mparallel_wavegan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMelGANGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "from parallel_wavegan.models import MelGANGenerator\n",
    "from parallel_wavegan.models.tf_models import TFMelGANGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DWtwgKZ9R8K"
   },
   "source": [
    "## Define Tensorflow and Pytorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wevXSH0n9R8L"
   },
   "outputs": [],
   "source": [
    "# load vocoder config \n",
    "vocoder_conf = 'ParallelWaveGAN/egs/ljspeech/voc1/conf/melgan.v1.long.yaml'\n",
    "with open(vocoder_conf) as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "XOK6AuWW9R8N",
    "outputId": "a60edb70-d9d0-4fe3-bf0f-7b0b2bda4fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 80)]        0         \n",
      "_________________________________________________________________\n",
      "tf_mel_gan_generator (TFMelG (None, None, 1)           4260257   \n",
      "=================================================================\n",
      "Total params: 4,260,257\n",
      "Trainable params: 4,260,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define Tensorflow MelGAN generator\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "inputs = tf.keras.Input(batch_shape=[None, None, 80], dtype=tf.float32)\n",
    "audio = TFMelGANGenerator(**config[\"generator_params\"])(inputs)\n",
    "tf_melgan = tf.keras.models.Model(inputs, audio)\n",
    "tf_melgan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wESkPTHZ9R8Q"
   },
   "outputs": [],
   "source": [
    "# define pytorch model\n",
    "pytorch_melgan = MelGANGenerator(**config[\"generator_params\"])\n",
    "pytorch_melgan.remove_weight_norm()  # needed since TFMelGANGenerator does not support weight norm\n",
    "pytorch_melgan = pytorch_melgan.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "q2PNLx9m9R8S",
    "outputId": "f5ecd5a8-fffe-44e2-c425-d2596ae4a85f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Tensorflow variables:  84\n",
      "Number Pytorch variables:  84\n"
     ]
    }
   ],
   "source": [
    "# check the number of variables are the same\n",
    "state_dict = pytorch_melgan.state_dict()\n",
    "tf_vars = tf.compat.v1.global_variables()\n",
    "print(\"Number Tensorflow variables: \", len(tf_vars))\n",
    "print(\"Number Pytorch variables: \", len(state_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4Ksgn1o9R8U"
   },
   "source": [
    "## Convert parameters from pytorch to tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzhiUmpv9R8V"
   },
   "outputs": [],
   "source": [
    "def reorder_tf_vars(tf_vars):\n",
    "    \"\"\"\n",
    "    Reorder tensorflow variables to match with pytorch state dict order. \n",
    "    Since each tensorflow layer's order is bias -> weight while pytorch's \n",
    "    one is weight -> bias, we change the order of variables.\n",
    "    \"\"\"\n",
    "    tf_new_var = []\n",
    "    for i in range(0, len(tf_vars), 2):\n",
    "        tf_new_var.append(tf_vars[i + 1])\n",
    "        tf_new_var.append(tf_vars[i])\n",
    "    return tf_new_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cono0O8n9R8X"
   },
   "outputs": [],
   "source": [
    "# change the order of variables to be the same as pytorch\n",
    "tf_vars = reorder_tf_vars(tf_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qNC78Bgy9R8Z"
   },
   "outputs": [],
   "source": [
    "def convert_weights_pytorch_to_tensorflow(weights_pytorch):\n",
    "    \"\"\"\n",
    "    Convert pytorch Conv1d weight variable to tensorflow Conv2D weights.\n",
    "    Pytorch (f_output, f_input, kernel_size) -> TF (kernel_size, f_input, 1, f_output)\n",
    "    \"\"\"\n",
    "    weights_tensorflow = np.transpose(weights_pytorch, (0,2,1))  # [f_output, kernel_size, f_input]\n",
    "    weights_tensorflow = np.transpose(weights_tensorflow, (1,0,2))  # [kernel-size, f_output, f_input]\n",
    "    weights_tensorflow = np.transpose(weights_tensorflow, (0,2,1))  # [kernel-size, f_input, f_output]\n",
    "    weights_tensorflow = np.expand_dims(weights_tensorflow, 1)  # [kernel-size, f_input, 1, f_output]\n",
    "    return weights_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hkciz2H49R8b"
   },
   "outputs": [],
   "source": [
    "# convert pytorch's variables to tensorflow's one\n",
    "for i, var_name in enumerate(state_dict):\n",
    "    try:\n",
    "        tf_name = tf_vars[i]\n",
    "        torch_tensor = state_dict[var_name].numpy()\n",
    "        if torch_tensor.ndim >= 2:\n",
    "            tensorflow_tensor = convert_weights_pytorch_to_tensorflow(torch_tensor)\n",
    "        else:\n",
    "            tensorflow_tensor = torch_tensor\n",
    "        tf.keras.backend.set_value(tf_name, tensorflow_tensor)\n",
    "    except:\n",
    "        print(tf_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VL0YFaji9R8d"
   },
   "source": [
    "## Check both outputs are almost the equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8FAomYs9R8e"
   },
   "outputs": [],
   "source": [
    "fake_mels = np.random.sample((1, 80, 250)).astype(np.float32)\n",
    "with torch.no_grad():\n",
    "    y_pytorch = pytorch_melgan(torch.Tensor(fake_mels))\n",
    "y_tensorflow = tf_melgan.predict(np.transpose(fake_mels, (0, 2, 1)))\n",
    "np.testing.assert_almost_equal(\n",
    "    y_pytorch[0, 0, :].numpy(),\n",
    "    y_tensorflow[0, :, 0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jy0c0qv39R8g"
   },
   "source": [
    "## Save Tensorflow and Pytorch models for benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ItFZ65Jn9R8g",
    "outputId": "dc573931-0485-44e4-b230-dbd37c0b6d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint/tensorflow_generator/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./checkpoint/tensorflow_generator/assets\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"./checkpoint/tensorflow_generator/\", exist_ok=True)\n",
    "os.makedirs(\"./checkpoint/pytorch_generator/\", exist_ok=True)\n",
    "tf.saved_model.save(tf_melgan, \"./checkpoint/tensorflow_generator/\")\n",
    "torch.save(pytorch_melgan.state_dict(), \"./checkpoint/pytorch_generator/checkpoint.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BsqIdyu9R8i"
   },
   "source": [
    "## Inference speed benchmark on GPU\n",
    "\n",
    "From here, we will compare the inference speed using pytorch model and converted tensorflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FU_NIGsu9R8j"
   },
   "outputs": [],
   "source": [
    "# To enable eager mode, we need to restart the runtime\n",
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PbcqANV9R8l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import convert_to_constants\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from parallel_wavegan.models import MelGANGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gg-E2hePEwTB"
   },
   "outputs": [],
   "source": [
    "# setup pytorch model\n",
    "vocoder_conf = 'ParallelWaveGAN/egs/ljspeech/voc1/conf/melgan.v1.long.yaml'\n",
    "with open(vocoder_conf) as f:\n",
    "    config = yaml.load(f, Loader=yaml.Loader)\n",
    "pytorch_melgan = MelGANGenerator(**config[\"generator_params\"])\n",
    "pytorch_melgan.remove_weight_norm()\n",
    "pytorch_melgan.load_state_dict(torch.load(\n",
    "    \"./checkpoint/pytorch_generator/checkpoint.pkl\", map_location=\"cpu\"))\n",
    "pytorch_melgan = pytorch_melgan.to(\"cuda\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OVUMAHuT9R8n"
   },
   "outputs": [],
   "source": [
    "# setup tensorflow model\n",
    "class TFMelGAN(object):\n",
    "    def __init__(self, saved_path):\n",
    "        self.saved_path = saved_path\n",
    "        self.graph = self._load_model()\n",
    "        self.mels = None\n",
    "        self.audios = None\n",
    "    \n",
    "    def _load_model(self):\n",
    "        saved_model_loaded = tf.saved_model.load(\n",
    "            self.saved_path, tags=[tag_constants.SERVING])\n",
    "        graph_func = saved_model_loaded.signatures[\n",
    "            signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
    "        graph_func = convert_to_constants.convert_variables_to_constants_v2(graph_func)\n",
    "        return graph_func\n",
    "\n",
    "    def set_mels(self, values):\n",
    "        self.mels = tf.identity(tf.constant(values))\n",
    "\n",
    "    def get_mels(self):\n",
    "        return self.mels\n",
    "\n",
    "    def get_audio(self):\n",
    "        return self.audios\n",
    "\n",
    "    def run_inference(self):\n",
    "        self.audios = self.graph(self.mels)[0]\n",
    "        return self.audios   \n",
    "    \n",
    "tf_melgan = TFMelGAN(saved_path='./checkpoint/tensorflow_generator/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XC7Y0H7r9R8p"
   },
   "outputs": [],
   "source": [
    "# warmup\n",
    "fake_mels = np.random.sample((4, 1500, 80)).astype(np.float32)\n",
    "tf_melgan.set_mels(fake_mels)\n",
    "fake_mels = torch.Tensor(fake_mels).transpose(2, 1).to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    y = pytorch_melgan(fake_mels)\n",
    "y = tf_melgan.run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "colab_type": "code",
    "id": "CrgQbMYWXDQy",
    "outputId": "3c7c0a23-39f0-458a-cf34-4a47a705802a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 29 12:57:18 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P0    37W / 250W |   5903MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "GhKgA5pk9R8r",
    "outputId": "945ccc65-0bfd-4a7e-9db0-1f6df7c4ab5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.71 ms, sys: 275 µs, total: 8.99 ms\n",
      "Wall time: 11.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# check pytorch inference speed\n",
    "with torch.no_grad():\n",
    "    y = pytorch_melgan(fake_mels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "vYZR0ihS9R8s",
    "outputId": "16f22663-acb0-44cf-f84a-38dfd0df406b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.62 ms, sys: 918 µs, total: 7.54 ms\n",
      "Wall time: 10.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# check tensorflow inference speed\n",
    "y = tf_melgan.run_inference()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Convert MelGAN generator from pytorch to tensorflow",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
